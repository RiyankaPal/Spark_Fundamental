# What is Apache Spark?

**Apache Spark** is a unified computing engine and a set of libraries for parallel data processing on a computer cluster.

---

## 🔍 Let’s break down this definition:

### 1. Unified

“Unified” means that data analysts, data scientists, and data engineers can all use the same platform for various tasks like:

- Data analysis  
- Data transformation  
- Modeling  

This allows for seamless collaboration and a consistent tool across different roles.

---

### 2. Computing Engine

Spark does **not store data** by itself. Instead, it performs computations (like addition, multiplication, etc.) on data.

To store data, Spark provides connectors to various data sources, such as:

- HDFS  
- S3  
- Databases  

💡 *Compute* refers to **processing tasks** — not storing data, just doing operations on it.

---

### 3. Parallel Data Processing

Parallel processing means dividing a large task into smaller sub-tasks and executing them **simultaneously** on multiple processors.

**Example:**  
Instead of using a single processor to search 1 million records, Spark divides the task:

- One processor searches the first 500,000 records.  
- Another processor searches the next 500,000 records.  

✅ This significantly **speeds up** the process.

---

### 4. Computer Cluster

A **cluster** is a group of interconnected computers, where:

- One computer acts as the **master** (coordinator)  
- Other computers act as **slaves** (workers)  

Spark distributes the computation tasks across all nodes in the cluster, allowing it to **scale horizontally** and handle large datasets efficiently.

---

## 🧠 In Summary:

Apache Spark is a powerful framework for performing **parallel data processing** across multiple machines (a cluster), **without handling storage** itself.  

Its **unified nature** allows users across roles to perform various tasks seamlessly, making it a **go-to solution for big data processing**.

---

## 💡 Why is Apache Spark Needed?

Earlier, we mainly handled data in **tabular formats** using traditional databases like **MySQL** or **Oracle**.  
But over time, data started coming in various formats such as:

- **Structured data** — tables in databases  
- **Semi-structured data** — JSON, XML, YAML  
- **Unstructured data** — text files, images, videos, logs, etc.  

These traditional systems could only handle **structured data well** and struggled with others.

---

## 🌊 The Rise of Big Data

**Big Data** is data that is too **large** and **complex** for traditional tools to process efficiently.

It is defined by the **3 Vs**:

- **Volume** — amount of data (e.g., 5GB, 5TB)  
- **Velocity** — speed of data (e.g., 1 second, 1 hour)  
- **Variety** — forms of data (structured, unstructured, semi-structured)  

> ⚠️ Data is not considered “big” just because of size. The **speed** and **type** of data also matter.  
> For example, if 5GB is generated per **second** and includes **multiple formats**, it qualifies as **Big Data**.

---

## 🔄 ETL → ELT

To handle this complexity, the traditional **ETL (Extract, Transform, Load)** approach evolved into **ELT (Extract, Load, Transform)** — especially with the advent of **Data Lakes**.

- ELT allows raw data to be **stored first** and **transformed later**, making systems more **flexible** for large volumes and varied data.

---

## 🧩 Challenges with Big Data

Two major challenges:

- **Storage** — Storing such large and varied datasets  
- **Processing** — The need for significant **RAM** and **CPU** resources  

---

## 🧱 Monolithic vs Distributed Approach

To address these, we had two primary approaches:

**Monolithic libraries** are designed to run on a single machine whereas **distributed libraries** are designed to run on multiple machines. Hence monolithic libraries rely on vertical scaling (increasing the capacity of a single machine by adding more CPU, RAM, or storage to the existing machine), whereas distributed libraries rely on horizontal scaling (adding more machines to a system and distributing the load across multiple nodes).

✅ We choose Distributed approach, Due to the nature of Big Data, where data is vast and growing rapidly, a distributed approach was necessary. This approach allows us to spread the data across multiple machines, thus providing:

Horizontal scalability (adding more machines instead of just upgrading a single machine)
Efficient data processing without overwhelming a single system’s resources (CPU, RAM)<br>

## 🔄 From Hadoop to Spark 
Previously, we worked with Hadoop, but in the next section, we’ll explore the differences between Hadoop and Spark to understand why Spark became the framework of choice for many.

## 🚀 Enter Apache Spark
This is where Apache Spark comes in. Spark is a distributed processing framework that efficiently handles Big Data across a cluster of machines.

---





