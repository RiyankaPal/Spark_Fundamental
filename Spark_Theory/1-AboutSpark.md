# What is Apache Spark?

**Apache Spark** is a unified computing engine and a set of libraries for parallel data processing on a computer cluster.

---

## ðŸ” Letâ€™s break down this definition:

### 1. Unified

â€œUnifiedâ€ means that data analysts, data scientists, and data engineers can all use the same platform for various tasks like:

- Data analysis  
- Data transformation  
- Modeling  

This allows for seamless collaboration and a consistent tool across different roles.

---

### 2. Computing Engine

Spark does **not store data** by itself. Instead, it performs computations (like addition, multiplication, etc.) on data.

To store data, Spark provides connectors to various data sources, such as:

- HDFS  
- S3  
- Databases  

ðŸ’¡ *Compute* refers to **processing tasks** â€” not storing data, just doing operations on it.

---

### 3. Parallel Data Processing

Parallel processing means dividing a large task into smaller sub-tasks and executing them **simultaneously** on multiple processors.

**Example:**  
Instead of using a single processor to search 1 million records, Spark divides the task:

- One processor searches the first 500,000 records.  
- Another processor searches the next 500,000 records.  

âœ… This significantly **speeds up** the process.

---

### 4. Computer Cluster

A **cluster** is a group of interconnected computers, where:

- One computer acts as the **master** (coordinator)  
- Other computers act as **slaves** (workers)  

Spark distributes the computation tasks across all nodes in the cluster, allowing it to **scale horizontally** and handle large datasets efficiently.

---

## ðŸ§  In Summary:

Apache Spark is a powerful framework for performing **parallel data processing** across multiple machines (a cluster), **without handling storage** itself.  

Its **unified nature** allows users across roles to perform various tasks seamlessly, making it a **go-to solution for big data processing**.

---

## ðŸ’¡ Why is Apache Spark Needed?

Earlier, we mainly handled data in **tabular formats** using traditional databases like **MySQL** or **Oracle**.  
But over time, data started coming in various formats such as:

- **Structured data** â€” tables in databases  
- **Semi-structured data** â€” JSON, XML, YAML  
- **Unstructured data** â€” text files, images, videos, logs, etc.  

These traditional systems could only handle **structured data well** and struggled with others.

---

## ðŸŒŠ The Rise of Big Data

**Big Data** is data that is too **large** and **complex** for traditional tools to process efficiently.

It is defined by the **3 Vs**:

- **Volume** â€” amount of data (e.g., 5GB, 5TB)  
- **Velocity** â€” speed of data (e.g., 1 second, 1 hour)  
- **Variety** â€” forms of data (structured, unstructured, semi-structured)  

> âš ï¸ Data is not considered â€œbigâ€ just because of size. The **speed** and **type** of data also matter.  
> For example, if 5GB is generated per **second** and includes **multiple formats**, it qualifies as **Big Data**.

---

## ðŸ”„ ETL â†’ ELT

To handle this complexity, the traditional **ETL (Extract, Transform, Load)** approach evolved into **ELT (Extract, Load, Transform)** â€” especially with the advent of **Data Lakes**.

- ELT allows raw data to be **stored first** and **transformed later**, making systems more **flexible** for large volumes and varied data.

---

## ðŸ§© Challenges with Big Data

Two major challenges:

- **Storage** â€” Storing such large and varied datasets  
- **Processing** â€” The need for significant **RAM** and **CPU** resources  

---

## ðŸ§± Monolithic vs Distributed Approach

To address these, we had two primary approaches:

**Monolithic libraries** are designed to run on a single machine whereas **distributed libraries** are designed to run on multiple machines. Hence monolithic libraries rely on vertical scaling (increasing the capacity of a single machine by adding more CPU, RAM, or storage to the existing machine), whereas distributed libraries rely on horizontal scaling (adding more machines to a system and distributing the load across multiple nodes).

âœ… We choose Distributed approach, Due to the nature of Big Data, where data is vast and growing rapidly, a distributed approach was necessary. This approach allows us to spread the data across multiple machines, thus providing:

Horizontal scalability (adding more machines instead of just upgrading a single machine)
Efficient data processing without overwhelming a single systemâ€™s resources (CPU, RAM)<br>

## ðŸ”„ From Hadoop to Spark 
Previously, we worked with Hadoop, but in the next section, weâ€™ll explore the differences between Hadoop and Spark to understand why Spark became the framework of choice for many.

## ðŸš€ Enter Apache Spark
This is where Apache Spark comes in. Spark is a distributed processing framework that efficiently handles Big Data across a cluster of machines.

---





